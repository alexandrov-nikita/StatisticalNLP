{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group exercise 13.02:\n",
    "- Load the dataset Product Review Dataset for Sentiment Analysis from http://people.mpi-inf.mpg.de/~smukherjee/data/\n",
    "- Do necessary preprocessing of the text lemmatization and stemming. Decide how you would deal with the apostrophes (nâ€™t => not) and other special symbols.\n",
    "https://drive.google.com/open?id=1A-N8MpBfpMBnJhSeWx2YoU84a1poAVur\n",
    "- Use page 6 to observe a variety of decisions need to be made while doing     preprocessing https://www.nyu.edu/projects/spirling/documents/preprocessing.pdf\n",
    "- Inspect the vocabulary and further decrease the size of it by matching words like USA and U.S.A., etc.\n",
    "- Find collocations using PointWise Mutual Information and t-test metrics (15.3.1) of the bigram\n",
    "- For each unigram and bigram find the probability in the corpus\n",
    "- Implement add-one smoothing, discounting, back-off and stupid back-off, interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'feature-specific/Dataset2.txt'\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "unigrams, bigrams = FreqDist(), FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, tokenizer=tokenizer, stemmer=stemmer):\n",
    "    '''Preprocesses a line and returns a list of preprocessed tokens'''\n",
    "\n",
    "    # lower case and remove leading/trailing spaces\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    \n",
    "    # apply stemmer to each token\n",
    "    tokens = list(map(stemmer.stem, tokens))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, mode='r') as f:\n",
    "    \n",
    "    for line in f.readlines():\n",
    "\n",
    "        # format: category $ label $ review\n",
    "        parts = line.split('$')\n",
    "        \n",
    "        review = parts[2]\n",
    "        tokens = preprocess(review)\n",
    "        \n",
    "        finder = BigramCollocationFinder.from_words(tokens)\n",
    "        \n",
    "        # add unigrams and bigrams of each review to global unigrams and bigrams\n",
    "        unigrams += finder.word_fd\n",
    "        bigrams += finder.ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a BigramCollocationFinder out of all the unigrams and bigrams\n",
    "# we found from the product reviews\n",
    "finder = BigramCollocationFinder(unigrams, bigrams)\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "pmi_scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "t_scored = finder.score_ngrams(bigram_measures.student_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmi scores:\n",
      "('10m', 'ethernet') 16.04180885427799\n",
      "('3m', '4m') 16.04180885427799\n",
      "('640', '30fps') 16.04180885427799\n",
      "('65536', '16bit') 16.04180885427799\n",
      "('abandon', 'thier') 16.04180885427799\n",
      "('accentu', 'dimish') 16.04180885427799\n",
      "('achill', 'heel') 16.04180885427799\n",
      "('afro', 'cuban') 16.04180885427799\n",
      "('agk', 'studio') 16.04180885427799\n",
      "('anni', 'lebovitz') 16.04180885427799\n",
      "('bose', 'quietcomfort') 16.04180885427799\n",
      "('bothersom', 'alleg') 16.04180885427799\n",
      "('brief', 'synopsi') 16.04180885427799\n",
      "('bulbous', 'formless') 16.04180885427799\n",
      "('cdr', 'cdrw') 16.04180885427799\n",
      "('cdrw', 'dvdr') 16.04180885427799\n",
      "('ceram', 'tile') 16.04180885427799\n",
      "('comress', 'smaler') 16.04180885427799\n",
      "('cordless', 'drill') 16.04180885427799\n",
      "('cosmet', 'standpoint') 16.04180885427799\n"
     ]
    }
   ],
   "source": [
    "print('pmi scores:')\n",
    "for bigram, score in pmi_scored[:20]:\n",
    "    print(bigram, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the top candidates according to the pmi score contain rare words. Maybe even c(w1) = c(w1, w2) = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-scores:\n",
      "('it', 's') 12.884295578726505\n",
      "('i', 'have') 12.812256227650057\n",
      "('of', 'the') 12.605367250001615\n",
      "('easi', 'to') 11.967589633694583\n",
      "('n', 't') 11.956585688767547\n",
      "('if', 'you') 11.918357796787728\n",
      "('to', 'use') 11.746838969167108\n",
      "('it', 'is') 11.182119745380465\n",
      "('you', 'can') 10.976127688906095\n",
      "('i', 've') 10.434353257186\n",
      "('is', 'a') 10.292449565083063\n",
      "('on', 'the') 10.246031139677918\n",
      "('this', 'is') 9.72287405475398\n",
      "('this', 'camera') 9.650076070474315\n",
      "('i', 'm') 9.182969523737354\n",
      "('this', 'phone') 9.13843545620724\n",
      "('the', 'ipod') 9.07075862237984\n",
      "('to', 'be') 9.018155296082693\n",
      "('with', 'the') 8.972702920561462\n",
      "('i', 'am') 8.933835053570371\n"
     ]
    }
   ],
   "source": [
    "print('t-scores:')\n",
    "for bigram, score in t_scored[:20]:\n",
    "    print(bigram, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top candidates here seem to be bigrams that appear relatively often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement add-one smoothing, discounting, back-off and stupid back-off, interpolation\n",
    "\n",
    "TODO\n",
    "\n",
    "Implement just a function that calculates the probability or also compute the required values to calculate the probability? Should we apply this somehow?\n",
    "\n",
    "Discounting and back-off are just concepts rather than something one can just implement?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
