{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../tweets/en/all.csv'\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, tokenizer=tokenizer, stemmer=stemmer):\n",
    "    '''Preprocesses a line and returns a list of preprocessed tokens'''\n",
    "    \n",
    "    # lower case and remove leading/trailing spaces\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in text.split():\n",
    "        # skip links and user handles\n",
    "        if token.startswith('https://') \\\n",
    "        or token.startswith('@') \\\n",
    "        or token in stopwords.words('english'):\n",
    "            continue\n",
    "            \n",
    "        tokens.append(token)\n",
    "    \n",
    "    # tokenize the text\n",
    "    tokens = tokenizer.tokenize(' '.join(tokens))\n",
    "    \n",
    "    # apply stemmer to each token\n",
    "#     tokens = list(map(stemmer.stem, tokens))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "with open(path, 'r', newline='\\r\\n') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        n, tweet = line.split(',', maxsplit=1)\n",
    "        tokens = preprocess(tweet)\n",
    "        \n",
    "        tweets.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    (50, 5, 3, 0),\n",
    "    (150, 3, 3, 0),\n",
    "    (50, 2, 0, 0),\n",
    "    (50, 5, 3, 1),\n",
    "    (150, 3, 3, 1),\n",
    "    (50, 2, 0, 1),\n",
    "]\n",
    "\n",
    "models = [word2vec.Word2Vec(sentences=tweets, size=size, window=window, min_count=min_count, sg=sg) \n",
    "         for size, window, min_count, sg in params]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('americanism', 0.5397476553916931)\n",
      "('message', 0.5394257307052612)\n",
      "('crater', 0.5337719917297363)\n",
      "('basher', 0.5284423232078552)\n",
      "('1nrsmith', 0.5280888676643372)\n",
      "('almond', 0.5227072834968567)\n",
      "('whatever', 0.5168656706809998)\n",
      "('spamming', 0.5158582329750061)\n",
      "('slum', 0.5113565325737)\n",
      "('he', 0.5091564059257507)\n",
      "-----------------\n",
      "('almond', 0.6010277271270752)\n",
      "('thegoodfight', 0.5565204620361328)\n",
      "('monster', 0.5561071634292603)\n",
      "('confidants', 0.5517393946647644)\n",
      "('unconcerned', 0.5514921545982361)\n",
      "('russians', 0.551446259021759)\n",
      "('1nrsmith', 0.5481245517730713)\n",
      "('milne', 0.5474159121513367)\n",
      "('clans', 0.5381943583488464)\n",
      "('he', 0.5362292528152466)\n",
      "-----------------\n",
      "('admini', 0.7923679351806641)\n",
      "('lisambrauer', 0.7569103240966797)\n",
      "('forewarning', 0.7202783823013306)\n",
      "('spamming', 0.7110344767570496)\n",
      "('someone', 0.7107893824577332)\n",
      "('loverboy', 0.7075369954109192)\n",
      "('gutfeld', 0.7050368785858154)\n",
      "('glowing', 0.7023209929466248)\n",
      "('rusdia', 0.7011188864707947)\n",
      "('going2', 0.6998761892318726)\n",
      "-----------------\n",
      "('russia', 0.7137513160705566)\n",
      "('buddy', 0.688981294631958)\n",
      "('puppet', 0.6863270401954651)\n",
      "('chump', 0.682657778263092)\n",
      "('russians', 0.68055659532547)\n",
      "('vlad', 0.6782978773117065)\n",
      "('vladimir', 0.6693828105926514)\n",
      "('betray', 0.6677103638648987)\n",
      "('enemy', 0.6675880551338196)\n",
      "('monster', 0.6675043702125549)\n",
      "-----------------\n",
      "('vladimir', 0.5628381371498108)\n",
      "('puppet', 0.5482814311981201)\n",
      "('russians', 0.5473231077194214)\n",
      "('phone', 0.5381614565849304)\n",
      "('monster', 0.5376958250999451)\n",
      "('buddy', 0.536931574344635)\n",
      "('putins', 0.5358173251152039)\n",
      "('enemy', 0.5353907942771912)\n",
      "('chump', 0.5319194793701172)\n",
      "('djt', 0.5219914317131042)\n",
      "-----------------\n",
      "('russia', 0.7702038288116455)\n",
      "('phone', 0.7663390040397644)\n",
      "('vladimir', 0.7549150586128235)\n",
      "('russians', 0.7416875958442688)\n",
      "('monster', 0.7250266075134277)\n",
      "('publicly', 0.7247147560119629)\n",
      "('puppet', 0.7201650142669678)\n",
      "('priest', 0.7153886556625366)\n",
      "('whatever', 0.713539719581604)\n",
      "('erdogan', 0.7092196345329285)\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for x in model.wv.most_similar(positive=['putin']):\n",
    "        print(x)\n",
    "    print('-----------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
